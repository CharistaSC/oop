{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CharistaSC/oop/blob/main/Week13_DL_based_RS_and_MLOps.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Week 13 Lab: DL based RS and MLOps"
      ],
      "metadata": {
        "id": "R_e1yAgZYOb1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 1: Building a DL based RS"
      ],
      "metadata": {
        "id": "uLEkM4qKYSI9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-7NXNChXiZZ"
      },
      "outputs": [],
      "source": [
        "!pip install scikit-surprise"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Surprise is a Python scikit for building and analyzing recommender systems that deal with explicit rating data: https://surpriselib.com/"
      ],
      "metadata": {
        "id": "YzgkW5w9y6dA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from surprise import Dataset\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.utils import shuffle"
      ],
      "metadata": {
        "id": "uiBizRLfYHFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### We first need to define a data loader"
      ],
      "metadata": {
        "id": "ynVw5ZkgbnHQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Loader():\n",
        "    current = 0\n",
        "\n",
        "    def __init__(self, x, y, batch_size=2048, is_shuffle=True):\n",
        "        from sklearn.utils import shuffle\n",
        "        self.shuffle = shuffle\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        self.batch_size = batch_size\n",
        "        self.batches = # fill in\n",
        "\n",
        "        # If is_shuffle, we re-shuffle on every epoch\n",
        "        if is_shuffle:\n",
        "            self.x, self.y = # fill in\n",
        "\n",
        "    def __iter__(self):\n",
        "        # Reset and return a new iterator\n",
        "        self.x, self.y = shuffle(self.x, self.y, random_state=0)\n",
        "        self.current = 0\n",
        "        return self\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return the number of batches\n",
        "        return # fill in\n",
        "\n",
        "    def __next__(self):\n",
        "        n = self.batch_size\n",
        "\n",
        "        if self.current + n >= len(self.y):\n",
        "            raise StopIteration\n",
        "\n",
        "        i = self.current\n",
        "        xs = # fill in -- hint: using torch.from_numpy\n",
        "        ys = # fill in\n",
        "\n",
        "        self.current += n\n",
        "\n",
        "        return (xs, ys)"
      ],
      "metadata": {
        "id": "5cTMb35RaH44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let us build a Matrix Factorization (MF) model using PyTorch"
      ],
      "metadata": {
        "id": "1J-EFoeWbwCn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(Optional) Homework: You can try other models such as Multi-layerd Perceptrons (MLP), Neural Collaborative Filtering (NeuMF)"
      ],
      "metadata": {
        "id": "froPN3AI0K4N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MF(nn.Module):\n",
        "\n",
        "    def __init__(self, n_user, n_item, k=18, c_vector=1.0, c_bias=1.0):\n",
        "        super(MF, self).__init__()\n",
        "        self.k = k\n",
        "        self.n_user = n_user\n",
        "        self.n_item = n_item\n",
        "        self.c_bias = c_bias\n",
        "        self.c_vector = c_vector\n",
        "        \n",
        "        self.user = nn.Embedding(n_user, k)\n",
        "        self.item = nn.Embedding(n_item, k)\n",
        "        \n",
        "        # We add new terms here:\n",
        "        self.bias_user = # fill in\n",
        "        self.bias_item = # fill in\n",
        "\n",
        "        self.bias = nn.Parameter(torch.ones(1))\n",
        "    \n",
        "    def forward(self, train_x):\n",
        "        user_id = train_x[:, 0]\n",
        "        item_id = train_x[:, 1]\n",
        "\n",
        "        vector_user = # fill in\n",
        "        vector_item = # fill in\n",
        "        \n",
        "        # Pull out biases\n",
        "        bias_user = # fill in -- hint: remember to use .squeeze()\n",
        "        bias_item = # fill in\n",
        "\n",
        "        biases = # fill in\n",
        "        \n",
        "        ui_interaction = # fill in\n",
        "        \n",
        "        # Add bias prediction to the interaction prediction\n",
        "        prediction = # fill in\n",
        "        return prediction\n",
        "    \n",
        "    def loss(self, prediction, target):\n",
        "\n",
        "        def l2_regularize(array):\n",
        "            loss = torch.sum(array**2)\n",
        "            return loss\n",
        "\n",
        "        loss_mse = # fill in -- hint: using F.mse_loss and target.squeeze()\n",
        "        \n",
        "        # Add new regularization to the biases\n",
        "        prior_bias_user =  l2_regularize(self.bias_user.weight) * self.c_bias\n",
        "        prior_bias_item = l2_regularize(self.bias_item.weight) * self.c_bias\n",
        "        \n",
        "        prior_user =  l2_regularize(self.user.weight) * self.c_vector\n",
        "        prior_item = l2_regularize(self.item.weight) * self.c_vector\n",
        "        \n",
        "        total = # fill in\n",
        "        return total"
      ],
      "metadata": {
        "id": "IfUUVF5RaPRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Finally, we define the main() function"
      ],
      "metadata": {
        "id": "taLuezwkb3TO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Get data\n",
        "    data = Dataset.load_builtin('ml-1m')\n",
        "    trainset = data.build_full_trainset()\n",
        "    uir = np.array([x for x in trainset.all_ratings()])\n",
        "\n",
        "    train_x = test_x = uir[:,:2].astype(np.int64)\n",
        "    train_y = test_y = uir[:,2].astype(np.float32)\n",
        "\n",
        "    # Define parameters\n",
        "    lr = 1e-1 # learning rate\n",
        "    k = 10  # embedding size or latent dimension\n",
        "    c_bias = 1e-6\n",
        "    c_vector = 1e-6\n",
        "    batch_size = 2048\n",
        "\n",
        "    model = # fill in -- hint: call MF model\n",
        "    \n",
        "    optimizer = # fill in -- hint: use torch.optim.Adam\n",
        "    dataloader = # fill in -- hint: call Loader method\n",
        "\n",
        "    itr = 0\n",
        "    for batch in dataloader:\n",
        "\n",
        "        itr += 1\n",
        "\n",
        "        prediction = model(batch[0])\n",
        "        loss = model.loss(prediction,batch[1])\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        print(f\"iteration: {itr}. training loss: {loss}\")\n",
        "\n",
        "    torch.save(model.state_dict(), \"./recommendation_model_pytorch.pkl\")\n",
        "\n",
        "if __name__=='__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "FdShzDZnaVg3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Now we're ready to do inference"
      ],
      "metadata": {
        "id": "5so8LhMObdiN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For inference, we will use most of the code from before (especially the model definition)."
      ],
      "metadata": {
        "id": "Jcz4iSerc3G6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from surprise import Dataset\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import pprint"
      ],
      "metadata": {
        "id": "f1hR0Urk1x5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This function aims to get the top-N recommendations\n",
        "\n",
        "def get_top_n(model, testset, trainset, uid_input, movies_df, n=10):\n",
        "    \n",
        "    preds = []\n",
        "    try:\n",
        "        uid_input = int(trainset.to_inner_uid(uid_input))\n",
        "    except KeyError:\n",
        "        return preds        \n",
        "\n",
        "    # We first map the predictions to each user.\n",
        "    for uid, iid, _ in testset:\n",
        "        try:\n",
        "            uid_internal = int(trainset.to_inner_uid(uid))\n",
        "        except KeyError:\n",
        "            continue\n",
        "            \n",
        "        if uid_internal == uid_input:\n",
        "            try:\n",
        "                iid_internal = int(trainset.to_inner_iid(iid))\n",
        "                movie_name = movies_df.loc[int(iid), 'name']\n",
        "                preds.append(# fill in) -- hint: (item id, movie name, rating from model(torch.tensor([[uid_input, iid_internal]])))\n",
        "            except KeyError:\n",
        "                pass\n",
        "\n",
        "    # Then, we sort the predictions for each user and retrieve the k highest ones (i.e., top-K recommendations)\n",
        "    if preds is not None:\n",
        "        preds.sort(key=lambda x: x[1], reverse=True)\n",
        "        if len(preds) > n:\n",
        "            preds = # fill in\n",
        "    return preds"
      ],
      "metadata": {
        "id": "1oinPxkm10Ss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This function aism to get previously seen items (i.e., items that user has already seen in the past)\n",
        "\n",
        "def get_previously_seen(trainset, uid, movies_df):\n",
        "    seen = []\n",
        "    for (iid, _) in trainset.ur[int(uid)]:\n",
        "        try:\n",
        "            seen.append(# fill in)\n",
        "        except KeyError:\n",
        "            pass\n",
        "        if len(seen) > 10:\n",
        "            break\n",
        "    return seen"
      ],
      "metadata": {
        "id": "vyEKBuaq19fg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    data = Dataset.load_builtin('ml-1m')\n",
        "\n",
        "    import os\n",
        "    files_dir = os.path.expanduser(\"/root/.surprise_data/ml-1m/ml-1m/\")\n",
        "\n",
        "    movies_df = pd.read_csv(files_dir + 'movies.dat', sep=\"::\", header=None, engine='python', encoding='latin-1')\n",
        "    movies_df.columns = ['iid','name','genre']\n",
        "    movies_df.set_index('iid',inplace=True)\n",
        "\n",
        "    trainset = data.build_full_trainset()\n",
        "    testset = trainset.build_anti_testset()\n",
        "\n",
        "    k = 10 # embedding size or latent dimension\n",
        "    c_bias = 1e-6\n",
        "    c_vector = 1e-6\n",
        "\n",
        "    model = MF(trainset.n_users, trainset.n_items, k=k, c_bias=c_bias, c_vector=c_vector)\n",
        "    model.load_state_dict(torch.load('./recommendation_model_pytorch.pkl'))\n",
        "    model.eval()\n",
        "\n",
        "    # Let us print the recommended items for some sample users\n",
        "    sample_users = # fill in\n",
        "\n",
        "    for uid in sample_users:\n",
        "        \n",
        "        print('User:',uid)\n",
        "        print('\\n')\n",
        "\n",
        "        print('\\tSeen:')\n",
        "        seen = # fill in\n",
        "        pprint.pprint(seen)\n",
        "        print('\\n')\n",
        "\n",
        "        print('\\tRecommendations:')\n",
        "        recommended = # fill in\n",
        "        pprint.pprint([x[1] for x in recommended])\n",
        "        print('\\n')\n",
        "\n",
        "\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "fK_Z00QTbiYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 2: Model Serving and MLOps"
      ],
      "metadata": {
        "id": "OWiYXZhzwVqs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Flask is a micro web framework written in Python. Function decorators are used in Flask to achieve routes to functions mapping. We first show how a simple service works, and then show how to load a model (e.g., based on pytorch) and serve it as well."
      ],
      "metadata": {
        "id": "DGIV-AtioIIa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # load Flask \n",
        "import flask\n",
        "import time\n",
        "import os\n",
        "\n",
        "app = flask.Flask(__name__)"
      ],
      "metadata": {
        "id": "pBS__IFjoaIg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unlike running Flask on a local machine, Google Colab provides you with a Virtual Machine (VM) whose localhost:5000 (Server Address for Flask Server and Default port) cannot be accessed directly. Therefore, the solution is to expose this address to a public url. To do this, a simple piece of code must be run before starting your server."
      ],
      "metadata": {
        "id": "RNpLSn4HtHo1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.output import eval_js\n",
        "print(eval_js(\"google.colab.kernel.proxyPort(5000)\"))"
      ],
      "metadata": {
        "id": "faZYbpL3tBej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "\n",
        "# Load data \n",
        "data = Dataset.load_builtin('ml-1m')\n",
        "trainset = data.build_full_trainset()\n",
        "testset = trainset.build_anti_testset()\n",
        "\n",
        "files_dir = os.path.expanduser(\"/root/.surprise_data/ml-1m/ml-1m/\")\n",
        "\n",
        "movies_df = pd.read_csv(files_dir + 'movies.dat', sep=\"::\", header=None, engine='python', encoding='latin-1')\n",
        "movies_df.columns = ['iid','name','genre']\n",
        "movies_df.set_index('iid',inplace=True)\n",
        "\n",
        "# Load model\n",
        "k = 10\n",
        "c_bias = 1e-6\n",
        "c_vector = 1e-6\n",
        "\n",
        "model = # fill in\n",
        "model # fill in\n",
        "model.eval()\n",
        "\n",
        "print('Model and data preloading completed in ', time.time()-start_time)\n",
        "\n",
        "@app.route(\"/\", methods=[\"GET\"])\n",
        "def recommend():\n",
        "\n",
        "    data = {\"success\": False}\n",
        "\n",
        "    if \"uid\" in flask.request.args:\n",
        "\n",
        "        data['uid'] = str(flask.request.args['uid'])\n",
        "\n",
        "        try:\n",
        "            data['seen'] = get_previously_seen(trainset, data['uid'], movies_df)\n",
        "            recommended = get_top_n(model, testset, trainset, data['uid'], movies_df, n=10)\n",
        "            print(recommended)\n",
        "            data['recommended'] = [x[1] for x in recommended]\n",
        "            data[\"success\"] = True\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    return flask.jsonify(data)\n",
        "    \n",
        "# start the flask app, allow remote connections\n",
        "if __name__ == '__main__':\n",
        "    app.run(host='0.0.0.0')"
      ],
      "metadata": {
        "id": "brKgYYNfdJ1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we launched a Flask web app, how can we get the recommendation results of user id 100, 196, 200, etc.?"
      ],
      "metadata": {
        "id": "UroJWfe4t88Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(Optional) Homework: try to build another similar recommender system using Flask with Jester dataset: https://eigentaste.berkeley.edu/dataset/. You can also other use built-ind datasets available on surprise (https://surpriselib.com/)"
      ],
      "metadata": {
        "id": "NqUpooXH-CwD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# End of Lab!"
      ],
      "metadata": {
        "id": "1KlWO1cOvZpC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reference:\n",
        "\n",
        "[1] University of Illinois Chicago. MLOps 2020."
      ],
      "metadata": {
        "id": "BgQuXeA2wHbw"
      }
    }
  ]
}